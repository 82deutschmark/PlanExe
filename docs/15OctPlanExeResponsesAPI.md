## GPT-5 Streaming Integration Plan

### October 2025 Contract Update

* The FastAPI relay now mirrors the official Responses stream events: `response.created`, `response.output_text.delta`, `response.reasoning_summary_text.delta`, `response.output_json.delta`, `response.completed`, and `response.error`. After the terminal event the server emits a `final` envelope containing the `stream.finalResponse()` payload so the UI can capture usage and consolidated content in one place.
* Intake requests must only include the latest user turn when a `conversation` id is present. Never resend accumulated history or handcrafted transcripts—the Responses thread already preserves it and OpenAI bills the prior context automatically.
* Default `store: true` on streaming calls so the OpenAI dashboard retains the response objects unless the caller explicitly requests a private turn.

PlanExe still routes all GPT calls through the legacy Chat Completions client and forwards raw Luigi stdout over WebSockets, so the real-time reasoning stream from the Responses API never reaches the UI. The new Responses guide requires streaming with `reasoning.effort`, `reasoning.summary`, and `text.verbosity` set explicitly, otherwise no reasoning deltas are emitted.​:codex-file-citation[codex-file-citation]{line_range_start=235 line_range_end=255 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L235-L255"}​​:codex-file-citation[codex-file-citation]{line_range_start=130 line_range_end=199 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L130-L199"}​ To adopt the “GPT-5 mini primary / GPT-5 nano fallback” direction while showing live reasoning, we need coordinated backend, pipeline, and frontend changes outlined below. ### 1. Align the model catalog with “mini primary, nano fallback” `llm_config.json` still maps the UI’s `gpt-5-mini-2025-08-07` entry to the nano model, and the Luigi default is hard-coded to that ID.​:codex-file-citation[codex-file-citation]{line_range_start=1 line_range_end=26 path=llm_config.json git_url="https://github.com/82deutschmark/PlanExe/blob/ui/llm_config.json#L1-L26"}​​:codex-file-citation[codex-file-citation]{line_range_start=98 line_range_end=116 path=planexe/plan/run_plan_pipeline.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe/plan/run_plan_pipeline.py#L98-L116"}​ The form also labels it “Default: GPT-5 Nano,” so the intended hierarchy is inconsistent.​:codex-file-citation[codex-file-citation]{line_range_start=59 line_range_end=239 path=planexe-frontend/src/components/planning/PlanForm.tsx git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe-frontend/src/components/planning/PlanForm.tsx#L59-L239"}​ Fixing this ensures the executor starts with GPT-5 mini and automatically falls back to GPT-5 nano when needed. :::task-stub{title="Set GPT-5 mini as the primary model with GPT-5 nano fallback"} 1. Update `llm_config.json` so the `gpt-5-mini-2025-08-07` entry points at the actual mini SKU and add a separate `gpt-5-nano-2025-08-07` item with the next priority slot. 2. Adjust any priority ordering logic in `PlanExeLLMConfig`/`LLMInfo` so `get_llm_names_by_priority()` resolves to `[mini, nano, …]` for downstream callers.​:codex-file-citation[codex-file-citation]{line_range_start=73 line_range_end=134 path=planexe/llm_factory.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe/llm_factory.py#L73-L134"}​3. Refresh `PlanForm` labels/default selection to reflect the corrected IDs and clarify which entry is the designated fallback.​:codex-file-citation[codex-file-citation]{line_range_start=59 line_range_end=239 path=planexe-frontend/src/components/planning/PlanForm.tsx git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe-frontend/src/components/planning/PlanForm.tsx#L59-L239"}​4. Verify `ExecutePipeline.resolve_llm_models()` still honors the explicit selection and auto list after the config change.​:codex-file-citation[codex-file-citation]{line_range_start=5162 line_range_end=5179 path=planexe/plan/run_plan_pipeline.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe/plan/run_plan_pipeline.py#L5162-L5179"}​::: ### 2. Replace Chat Completions with Responses API streaming in `SimpleOpenAILLM` `SimpleOpenAILLM` invokes `client.chat.completions.create()` and fakes streaming by yielding the final response, so no reasoning deltas ever surface.​:codex-file-citation[codex-file-citation]{line_range_start=68 line_range_end=190 path=planexe/llm_util/simple_openai_llm.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe/llm_util/simple_openai_llm.py#L68-L190"}​ This class must switch to `client.responses.stream()` and enforce the reasoning/verbosity knobs from the guide.​:codex-file-citation[codex-file-citation]{line_range_start=130 line_range_end=199 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L130-L199"}​​:codex-file-citation[codex-file-citation]{line_range_start=235 line_range_end=255 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L235-L255"}​:::task-stub{title="Refactor SimpleOpenAILLM to use OpenAI Responses streaming"} 1. Replace the synchronous `chat.completions` calls with `responses.stream()` (and `responses.create()` for non-stream paths) while mapping LlamaIndex-style message arrays into the `input` shape described in the guide.​:codex-file-citation[codex-file-citation]{line_range_start=30 line_range_end=125 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L30-L125"}​2. Always pass `reasoning={"effort": "high","summary": "detailed"}` and `text={"verbosity": "high"}` for GPT-5 mini/nano requests so reasoning deltas emit reliably.​:codex-file-citation[codex-file-citation]{line_range_start=235 line_range_end=255 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L235-L255"}​3. Aggregate `response.reasoning_summary_text.delta`, `response.content_part.added`, and completion events to produce both streaming callbacks and a final object compatible with current pipeline consumers.​:codex-file-citation[codex-file-citation]{line_range_start=130 line_range_end=199 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L130-L199"}​4. Extend `set_last_attempt_tokens` call sites to capture `usage.output_tokens_details.reasoning_tokens` from `finalResponse` and stash them in the executor metadata.​:codex-file-citation[codex-file-citation]{line_range_start=200 line_range_end=230 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L200-L230"}​​:codex-file-citation[codex-file-citation]{line_range_start=153 line_range_end=209 path=planexe/llm_util/llm_executor.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe/llm_util/llm_executor.py#L153-L209"}​5. Keep OpenRouter compatibility by gating the new flow to OpenAI providers and documenting that non-OpenAI models remain non-streaming until their APIs support it. ::: ### 3. Emit structured LLM streaming events from the Luigi process Luigi tasks still call `RedlineGate.execute()` (and dozens of similar helpers) synchronously, with no channel to push intermediate reasoning out. To surface real-time deltas, inject a lightweight event emitter that each task can use without breaking the existing interfaces. :::task-stub{title="Instrument Luigi LLM calls to publish streaming events"} 1. Introduce a `StreamingEventEmitter` utility within `planexe/llm_util` that accepts `plan_id`, `stage`, and callback hooks (e.g., printing JSON lines prefixed with `LLM_STREAM:`). 2. Update `LLMExecutor` so, when a task supplies the emitter context, it forwards streaming callbacks from `SimpleOpenAILLM` into that emitter while preserving the final return semantics. 3. Teach representative task wrappers (e.g., `RedlineGate.execute`, `GovernancePhase3ImplPlan.execute`) to register their `stage` with the emitter before invoking the LLM, ensuring every reasoning delta carries enough metadata for the UI to categorize it. 4. Ensure emitter output is deterministic newline-delimited JSON so downstream log parsers can distinguish it from ordinary Luigi logs. ::: ### 4. Bridge streaming events through FastAPI’s WebSocket layer `PipelineExecutionService` currently forwards each stdout line to WebSocket clients as a generic “log” message.​:codex-file-citation[codex-file-citation]{line_range_start=372 line_range_end=407 path=planexe_api/services/pipeline_execution_service.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe_api/services/pipeline_execution_service.py#L372-L407"}​ It needs to recognize the new `LLM_STREAM` markers, parse the JSON, and broadcast a dedicated message type so the UI can render reasoning separately. :::task-stub{title="Forward parsed streaming events to WebSocket subscribers"} 1. Extend `_monitor_process_execution.read_stdout()` to detect the `LLM_STREAM` prefix, decode the payload, and emit `{"type":"llm_stream",...}` frames via `websocket_manager.broadcast_to_plan` without losing the original log line.​:codex-file-citation[codex-file-citation]{line_range_start=372 line_range_end=407 path=planexe_api/services/pipeline_execution_service.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe_api/services/pipeline_execution_service.py#L372-L407"}​2. Update `WebSocketMessage` unions on the frontend API client to include the new `llm_stream` shape, keeping backward compatibility with existing listeners.​:codex-file-citation[codex-file-citation]{line_range_start=120 line_range_end=196 path=planexe-frontend/src/lib/api/fastapi-client.ts git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe-frontend/src/lib/api/fastapi-client.ts#L120-L196"}​3. Adjust any logging/metrics that rely on raw Luigi stdout so they do not double-count the new structured lines. 4. Add basic error handling for malformed JSON so a single bad event doesn’t terminate the stream. ::: ### 5. Present real-time reasoning and content deltas in the UI The React monitor only watches for log/status strings and has no notion of streaming text.​:codex-file-citation[codex-file-citation]{line_range_start=92 line_range_end=188 path=planexe-frontend/src/components/monitoring/LuigiPipelineView.tsx git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe-frontend/src/components/monitoring/LuigiPipelineView.tsx#L92-L188"}​ Extend it to visualize the new message type and buffer reasoning/output separately, similar to the client example in the Responses guide.​:codex-file-citation[codex-file-citation]{line_range_start=365 line_range_end=399 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L365-L399"}​:::task-stub{title="Render GPT-5 reasoning/output streams in the monitoring UI"} 1. Enhance the Zustand planning/monitoring store to maintain per-task buffers for `reasoning` and `text` deltas arriving in `llm_stream` messages, keyed by plan ID and stage. 2. Update `WebSocketClient` consumers (`LuigiPipelineView`, `Terminal`, any detail panes) to listen for the new event type and push chunks into those buffers while preserving existing log behavior.​:codex-file-citation[codex-file-citation]{line_range_start=92 line_range_end=188 path=planexe-frontend/src/components/monitoring/LuigiPipelineView.tsx git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe-frontend/src/components/monitoring/LuigiPipelineView.tsx#L92-L188"}​3. Build a dedicated streaming panel (e.g., a collapsible card per active task) that shows live reasoning scrollback and synthesized output, resetting when a task completes or fails. 4. Consider throttling UI updates (e.g., requestAnimationFrame) to avoid rendering every token if the stream is extremely verbose. ::: ### 6. Persist reasoning summaries and token metrics `LLMInteraction` already has `response_metadata`, `input_tokens`, and `output_tokens`, but today tasks only store final JSON bodies after the stream finishes.​:codex-file-citation[codex-file-citation]{line_range_start=60 line_range_end=118 path=planexe_api/database.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe_api/database.py#L60-L118"}​​:codex-file-citation[codex-file-citation]{line_range_start=320 line_range_end=420 path=planexe/plan/run_plan_pipeline.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe/plan/run_plan_pipeline.py#L320-L420"}​ Capture the aggregated reasoning and token counts so operators can audit model behavior after the run. :::task-stub{title="Store aggregated reasoning/tokens for each LLM interaction"} 1. When the emitter signals completion, attach the concatenated reasoning text and token usage to `LLMInteraction.response_metadata` (e.g., `{reasoning_log, text_log, reasoning_tokens}`) before calling `update_llm_interaction`.​:codex-file-citation[codex-file-citation]{line_range_start=320 line_range_end=420 path=planexe/plan/run_plan_pipeline.py git_url="https://github.com/82deutschmark/PlanExe/blob/ui/planexe/plan/run_plan_pipeline.py#L320-L420"}​2. Ensure the FastAPI endpoints that expose interaction history (or artefacts) include these new fields so the UI can display them post-run. 3. Backfill or migrate existing records if needed (e.g., add nullable columns) and update Alembic migrations accordingly. 4. Add diagnostics in the logs when reasoning is unexpectedly empty to catch configuration regressions early. ::: ### 7. Documentation and regression coverage Once streaming is wired end-to-end, update developer docs so future contributors know how to work with the Responses API flow, and add smoke/regression tests where feasible. :::task-stub{title="Document and validate the new streaming pipeline"} 1. Expand `docs/RESPONSES.md` (or add a companion doc) with PlanExe-specific wiring notes, including how the emitter, WebSocket, and UI components interact, plus troubleshooting tips for missing deltas.​:codex-file-citation[codex-file-citation]{line_range_start=235 line_range_end=361 path=docs/RESPONSES.md git_url="https://github.com/82deutschmark/PlanExe/blob/ui/docs/RESPONSES.md#L235-L361"}​2. Add backend unit coverage for the emitter/parser (e.g., ensure `LLM_STREAM` lines are parsed and forwarded) and frontend tests that simulate `llm_stream` messages and verify state updates. 3. Update any manual QA scripts or test fixtures that previously assumed chat-completion payloads so they assert the new reasoning metadata is present. 4. Double-check pinned package versions (`openai==1.59.5`) remain compatible with `responses.stream()` and bump if the SDK requires a newer release.​:codex-file-citation[codex-file-citation]{line_range_start=45 line_range_end=79 path=pyproject.toml git_url="https://github.com/82deutschmark/PlanExe/blob/ui/pyproject.toml#L45-L79"}​::: Implementing these tasks will promote GPT-5 mini to the primary slot, stream GPT-5 reasoning in real time, and preserve full traces for audits—all while staying aligned with the Responses API guide.